---
title: "Data 624 Project 2"
author: "Mary Anna Kivenson"
date: "May 3, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(RANN)
library(mlbench)
library(tidyverse)
library(VIM)
library(corrplot)
library(DT)
```

## Data

```{r}
train <- read.csv("https://raw.githubusercontent.com/mkivenson/Predictive-Analytics/master/Project%202/Datasets/student_train.csv")
test <- read.csv("https://raw.githubusercontent.com/mkivenson/Predictive-Analytics/master/Project%202/Datasets/student_test.csv")
```

## Data Exploration

#### Summary

First, we take a look at a summary of the data. 

- There are missing values in all of the predictors except for `Pressure.Vacuum` and `Air.Pressurer`
- The target variabale, `PH`, has four missing values that should be removed from the training data
- All predictors except for `Brand.Code` are numeric - `Brand.Code` will be encoded into dummy variables

```{r}
summary(train)
train <- drop_na(train, "PH") #remove rows without a PH value
```


#### Correlation Plot

This correlation plot shows high multicollinearity in the dataset.

```{r}
corrplot(cor(subset(train, select = -c(Brand.Code)), use = "complete.obs"), method="color", type="lower", tl.col = "black", tl.srt = 5)
```

#### More Data Exploration

EDA HERE

#### More Data Exploration

EDA HERE


## Preprocessing

While performing data exploration, the need for data imputation and encoding was revealed. As part of the preprocessing, these steps will be completed.

#### Encoding

Brand Code is a categorical variable that must be encoded prior to imputation. For each level in the category, a dummy variable is created. Typically, one less predictor than categories is required. However, since there are many missing values in this column, a 0 in each dummy variable corresponds to missing data. 

```{r}
train$Brand.A <- ifelse(train$Brand.Code == 'A', 1, 0)
train$Brand.B <- ifelse(train$Brand.Code == 'B', 1, 0)
train$Brand.C <- ifelse(train$Brand.Code == 'C', 1, 0)
train$Brand.D <- ifelse(train$Brand.Code == 'D', 1, 0)
train <- subset(train, select = -c(Brand.Code))
datatable(train[c("Brand.A", "Brand.B", "Brand.C", "Brand.D")])
```

#### Train Test Split

```{r}
X_train <- subset(train, select = -c(PH))
y_train <- train$PH
X_test <- subset(test, select = -c(PH))
y_test <- test$PH
```

#### Visualizing Missing Data

The following plots provide a visualization of missing data. There does not seem to be a significant patten in the mising values, and none of the predictors are sparse (highest missing rate is 8%).

```{r}
aggr(train[,sapply(train, is.numeric)], col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(train), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
```


#### KNN Imputation

In order to fill missing data, knnImputation will be used. KNN imputation is unsupervised, meaning it does not require a target variable. A train test split was performed earlier so that only predictor data is used for imputation.

```{r}
result <- preProcess(X_train, method = c("knnImpute"), k = 10)
X_train <- predict(result, X_train)
```

## Linear Models


### Linear Regression

```{r}
lm <- train(X_train, y_train,
                method = "lm",
                tuneLength = 30,
                trControl = trainControl(method = "cv", 10))
lm
```


### Principal Component Analysis

```{r}
rlmPCA <- train(X_train, y_train,
                method = "rlm",
                preProcess = "pca",
                tuneLength = 30,
                trControl = trainControl(method = "cv", 10))
rlmPCA 
```


### Partial Least Squares

```{r}
plsTune <- train(X_train, y_train, 
                 method = "pls", 
                 tuneLength = 10,
                 trControl = trainControl(method = "cv"))
plsTune
```


### Ridge Regression

```{r}
ridgeGrid <- data.frame(.lambda = seq(0, .1, length = 5))
ridgeRegFit <- train(X_train, y_train,
                     method = "ridge", 
                     tuneGrid = ridgeGrid, 
                     trControl = trainControl(method = "cv")
                     #preProc = c("center", "scale")
                     )
ridgeRegFit 
```


### Lasso Regression

```{r}
lassomodel <- train(X_train, y_train,
               method = "lasso", 
               trControl = trainControl(method = "cv")
               )
lassomodel
```

### Elastic Net Regression

```{r}
enetGrid <- expand.grid(.lambda = c(0, 0.05, .3), 
                        .fraction = seq(.1, 1, length = 10))

enetTune <- train(X_train, y_train,
                  method = "enet",
                  tuneGrid = enetGrid,
                  trControl = trainControl(method = "cv")
                  #preProc = c("center", "scale")
                  )

enetTune
```


## Non-Linear Models




## Regression Trees




## Evaluation



## Conclusion
